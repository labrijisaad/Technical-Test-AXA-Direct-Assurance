{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e71cd225",
   "metadata": {},
   "source": [
    "<center><img src=\"https://is1-ssl.mzstatic.com/image/thumb/Purple122/v4/05/e7/67/05e76784-3364-b535-7e20-b3f4946a56b6/AppIcon-0-0-1x_U007emarketing-0-0-0-7-0-0-sRGB-0-0-0-GLES2_U002c0-512MB-85-220-0-0.png/434x0w.webp\" style=\"height:150px\"></center>\n",
    "\n",
    "<hr style=\"border-width:2px;border-color:red\">\n",
    "<center><h1>Test Technique Data Scientist</h1></center>\n",
    "<center><h2> Mesure de la dérive </h2></center>\n",
    "<hr style=\"border-width:2px;border-color:red\">\n",
    "\n",
    "# Contexte\n",
    "\n",
    "La **dérive** en apprentissage automatique (ou **\"drift\"** en anglais) fait référence à un **changement dans les données d'entrée ou de sortie d'un modèle** de machine learning **après son entraînement initial**. Cela peut se produire lorsque les données utilisées pour entraîner le modèle ne **reflètent plus la réalité**, ou lorsque **les conditions du problème changent de manière significative**.\n",
    "\n",
    "La dérive des données peut affecter la précision du modèle de manière significative et peut entraîner des erreurs coûteuses dans les prévisions ou les décisions basées sur le modèle. Par conséquent, **il est important de surveiller régulièrement les performances du modèle** et de le mettre à jour en fonction des nouveaux jeux de données ou des nouvelles conditions du problème.\n",
    "\n",
    "# Objectif\n",
    "\n",
    "Dans ce test, vous allez implémenter des métriques permettant de mesurer la dérive d'un modèle dans un usecase de **e-commerce** où nous essayons de prédire le **panier total** (variable **`TotalCart`** : chiffre d'affaire total d'un client sur la période donnée) des clients en fonction des variables suivantes:\n",
    "* **`Age`** : âge du client en années.\n",
    "* **`Seniority`** : ancienneté du client en années.\n",
    "* **`Orders`** : Nombre de commandes effectuées sur la période précédente.\n",
    "* **`Items`** : Nombre d'items commandés sur la période précédente.\n",
    "* **`AverageDiscount`** : Réduction moyenne accordée au client sur la période précédente en pourcentage.\n",
    "* **`TopCategory`** : Catégorie de produits favorite du client.\n",
    "* **`BrowsingTime`** : Temps total passé sur le site web sur la période précédente en secondes.\n",
    "* **`EmailsOpened`** : Nombre de mails marketing ouverts par le client sur la période précédente.\n",
    "* **`SupportInteractions`** : Nombre d'interactions que le client a eu avec le service client sur la période précédente.\n",
    "\n",
    "Le jeu de données est décomposé en 4 périodes correspondant aux 4 trimestres de l'année 2022. Vous trouverez les données correspondant à chaque période dans les fichiers **`period_0.csv`**, **`period_1.csv`**, ..., **`period_3.csv`**.\n",
    "\n",
    "Vous allez d'abord **entraîner et évaluer un modèle de machine learning** sur les données de la **période 0**. On supposera que ce modèle sera utilisé pour effectuer les prédictions de panier total sur toute l'année 2022.\n",
    "\n",
    "Ensuite, vous devrez **implémenter des métriques de dérive** qui vous seront données et **effectuer une analyse** de celle-ci.\n",
    "\n",
    "# Entraînement du modèle\n",
    "\n",
    "* **Entraînez et validez** deux ou trois modèles de votre choix sur les données de la période 0. \n",
    "\n",
    "**Conseils**\n",
    "\n",
    "* Vous serez évalué sur votre **rigueur** et non sur les performances du modèle.\n",
    "\n",
    "\n",
    "* Il n'est pas nécessaire de faire une analyse exploratoire des données.\n",
    "\n",
    "\n",
    "* Il n'est pas nécessaire de faire de recherche d'hyperparamètres optimaux non plus mais vous pouvez le faire si vous le souhaitez."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee460c1",
   "metadata": {},
   "source": [
    "#\n",
    "<hr style=\"border-width:2px;border-color:red\">\n",
    "<center><h1>Solution</h1></center>\n",
    "\n",
    "To effectively address the problem and provide a comprehensive response to the test, I suggest defining the objectives as follows. These objectives serve as reference points for the sections covered in the notebook:\n",
    "\n",
    "## Expected tasks:\n",
    "\n",
    "> * **`Obj 1`** : Train and validate two/three Machine Learning models on **period 0** data.\n",
    "> * **`Obj 1 bis`** : Test Winner Model on **period 1, .. , 3** data.\n",
    "> * **`Obj 2`** : Implement drift metrics for measuring model drift.\n",
    "> * **`Obj 3`** : Analyze the model drift.\n",
    "> * No need for exploratory data analysis.\n",
    "> * No need for hyperparameter optimization, but it can be done if desired.\n",
    "<hr style=\"border-width:2px;border-color:red\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c5edcb",
   "metadata": {},
   "source": [
    "### <p style=\"text-align:left\"><span style=\"color:green\">Importing Required `Libraries`</span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "addbbeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis libraries:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data visualization libraries:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'iframe'\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "\n",
    "# Other libraries:\n",
    "#from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Data science imports:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    ExtraTreesRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    BaggingRegressor,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, median_absolute_error\n",
    "\n",
    "# Configure pandas to display all columns:\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Render figures directly in the notebook:\n",
    "%matplotlib inline\n",
    "\n",
    "# Render higher resolution images:\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Ignore warning messages:\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d569bc6b",
   "metadata": {},
   "source": [
    "### <p style=\"text-align:left\"><span style=\"color:green\">Reading the `CSV Files`</span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff2c5b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the path to the directory where csv data is stored\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\")\n",
    "\n",
    "\n",
    "# Listing the different datasets on our data directory\n",
    "files = sorted(os.listdir(DATA_PATH))\n",
    "for file in files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36b38cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the datasets\n",
    "periods = [] # Here we will store the datsets\n",
    "for file in files:\n",
    "    file_path = os.path.join(DATA_PATH, file)\n",
    "    period = pd.read_csv(file_path)\n",
    "    periods.append(period)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f621f711",
   "metadata": {},
   "source": [
    "## <p style=\"text-align:center\"><span style=\"color:red\">Training a model on `period_0 data` ( Obj 1 )</span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab7cb6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "quarter_0 = periods[0] # This the dataset that contains period_0 data\n",
    "print(f'The \"period_0\" dataframe shape: {quarter_0.shape}')\n",
    "quarter_0.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d40398b",
   "metadata": {},
   "source": [
    "> * We have 100 records in total.\n",
    "> * We have 9 Features, the target column is **`TotalCart`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0eb38b",
   "metadata": {},
   "source": [
    "### <p style=\"text-align:left\"><span style=\"color:green\">Analysing the `Missingness`</span></p>\n",
    "> Avant d'entrer plus dans, dans les details, on analyse la qualité de nos données ( missing elements, data types )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d517b80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_count = quarter_0.isnull().sum()\n",
    "missing_percentage = (missing_count / len(quarter_0)) * 100\n",
    "missing_data = pd.DataFrame({'Missing Count': missing_count, \n",
    "                             'Missing Percentage': missing_percentage})\n",
    "\n",
    "missing_data.sort_values('Missing Count', ascending=False).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722440c3",
   "metadata": {},
   "source": [
    "> No Missing data to handle !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab162f3",
   "metadata": {},
   "source": [
    "### <p style=\"text-align:left\"><span style=\"color:green\">Satisticals `Analysis`</span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "474098d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "quarter_0.describe() #shows count, mean, std etc. for each column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f36fbb0",
   "metadata": {},
   "source": [
    "### <p style=\"text-align:left\"><span style=\"color:green\">Analysing `TotalCart` Distrubution</span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b4c0976",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))  \n",
    "sns.histplot(data=quarter_0, x=\"TotalCart\", color='skyblue', edgecolor='black', alpha=0.7)\n",
    "plt.xlabel(\"Total Cart Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Total Cart Value in Quarter 0\")\n",
    "plt.grid(True)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370b53fb",
   "metadata": {},
   "source": [
    "> Based on the **Distrubution plot**, it is evident that the **target variable follows a positive skew data distribution**. \n",
    "\n",
    "> As we **`focus on finding a good model for the period_0 data without extensive feature engineering`**, we'll skip the exploration of the best performing models.\n",
    "\n",
    "> To address the positive skewness in the continuous target variable, I've researched some possible solutions. The following sources provide valuable insights on the matter:\n",
    "> * [Regression: How to deal with positive skewness in continuous target variable](https://datascience.stackexchange.com/questions/65467/regression-how-to-deal-with-positive-skewness-in-continuous-target-variable)\n",
    "> * [Top 3 Methods for Handling Skewed Data](https://towardsdatascience.com/top-3-methods-for-handling-skewed-data-1334e0debf45)\n",
    ">> We can employ various techniques such as **`Logarithmic Transformation`**, **`Square Root Transformation`**, or **`Box-Cox Transformation`** to adjust the target variable's distribution. Additionally, it is worth considering **`model stacking`** and assessing **`feature independence`** as potential approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9afa33",
   "metadata": {},
   "source": [
    "### <p style=\"text-align:left\"><span style=\"color:green\">Feature `Engineering` ( Not Performed ) </span></p>\n",
    "\n",
    "Although **feature engineering** can be performed on our data by creating new features, we have chosen not to do so because our objective is to visualize the effect of **data drift** rather than **optimize the model's performance**.\n",
    "\n",
    "> * **`AverageTimePerOrder`**: The average time spent browsing per order. ( $AverageTimePerOrder = \\frac{BrowsingTime}{Orders}$ )\n",
    "> * **`AverageOrderValue`**: The average order value. ( $AverageOrderValue = \\frac{TotalCart}{Orders}$ )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409af310",
   "metadata": {},
   "source": [
    "### <p style=\"text-align:left\"><span style=\"color:green\">Handling the `Categorical columns`</span></p>\n",
    "> * Before **`training the model`**, it is necessary to handle the categorical features since models typically operate with numerical data. There are various approaches to address this issue, such as using **`OneHotEncoding`** or **`LabelEncoder`** to encode the categorical features.                                       \n",
    "> * In this case, I have opted to use the **`LabelEncoder`** to encode the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbb7a249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the Categorical Features\n",
    "quarter_0.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f388f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the Unique Elements of the 'TopCategory' Feature\n",
    "quarter_0['TopCategory'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ec0cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverting the labels into a numeric form using Label Encoder\n",
    "le = LabelEncoder()\n",
    "for col in quarter_0.columns:\n",
    "    if quarter_0[col].dtype=='object':\n",
    "        quarter_0[col] = le.fit_transform(quarter_0[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "076f43ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "quarter_0.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94624412",
   "metadata": {},
   "outputs": [],
   "source": [
    "quarter_0['TopCategory'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eb5178",
   "metadata": {},
   "source": [
    "> As expected, the **`TopCategory`** was encodded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b59af1",
   "metadata": {},
   "source": [
    "### <p style=\"text-align:left\"><span style=\"color:green\">Analysing the `Correlation`</span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00cbaefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To enhance the visualization of the confusion matrix, we can define a list to reorder the columns accordingly.\n",
    "ordred_cols = ['Age', 'Seniority', 'Orders', 'Items', 'AverageDiscount',\n",
    "               'TopCategory', 'BrowsingTime', 'EmailsOpened', 'SupportInteractions', 'TotalCart'] \n",
    "\n",
    "quarter_0 = quarter_0[ordred_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b49eea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 15))\n",
    "corr = quarter_0.corr(method='pearson')\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "sns.heatmap(corr, annot=True, mask=mask, cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73acd503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the target variable\n",
    "target = 'TotalCart'\n",
    "\n",
    "# Defining the Correlation Threshold\n",
    "correlation_threshold = 0.4\n",
    "\n",
    "# Selecting the Highly Correlated Features \n",
    "highly_correlated_features = corr[corr[target] >= correlation_threshold].index.to_list()\n",
    "highly_correlated_features.remove(target)\n",
    "\n",
    "# Diplaying the selected Features\n",
    "print(f'The highly correlated variables with the \"{target}\" varible are: {highly_correlated_features}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a43d56a",
   "metadata": {},
   "source": [
    "### <p style=\"text-align:left\"><span style=\"color:green\">General `Insights`</span></p>\n",
    "Here are the key insights we have gathered so far about our regression ML problem:\n",
    "\n",
    "> * The dataset consists of 1000 rows and includes 8 numerical features and 1 categorical feature that has been label encoded for numerical representation.\n",
    "> * The target variable exhibits a right-skewed distribution with an outlier located at the beginning.\n",
    "> * Two features in the dataset demonstrate a strong correlation with the target variable.\n",
    "> * Importantly, the two highly correlated variables are not collinear, indicating they provide distinct information to the regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542b1c34",
   "metadata": {},
   "source": [
    "### <p style=\"text-align:left\"><span style=\"color:green\">Model `Training`</span></p>\n",
    "\n",
    "> Here's how I propose to approach this section:  \n",
    ">* First, I will test multiple models and select the **top three performing ones**. \n",
    ">* Next, I will subject them to **`K-Fold Cross Validation`** to ensure that the models are functioning properly and that the performance metrics are not biased by the choice of split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a14bc61",
   "metadata": {},
   "source": [
    "#### <p style=\"text-align:left\"><span style=\"color:blue\">Testing `Multiple Regression models`</span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bfb4c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the Dependent Variables in X and Independent Variable in Y\n",
    "target = 'TotalCart'\n",
    "features = highly_correlated_features\n",
    "test_size = 0.15\n",
    "\n",
    "x = quarter_0.drop(target, axis=1)\n",
    "y = quarter_0[target]\n",
    "\n",
    "# Splitting the Data into Training set and Testing Set\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=42)\n",
    "\n",
    "# Display shape information for training and test datasets\n",
    "print(f\"Training data:\\nX shape: {x_train.shape}\\nY shape: {y_train.shape}\\n\")\n",
    "print(f\"Test data:\\nX shape: {x_test.shape}\\nY shape: {y_test.shape}\\n\")\n",
    "\n",
    "\n",
    "# Define models\n",
    "models = [\n",
    "    LinearRegression(), DecisionTreeRegressor(), BaggingRegressor(), RandomForestRegressor(),\n",
    "    SVR(), KNeighborsRegressor(n_neighbors=5), Ridge(), Lasso(alpha=0.1),\n",
    "    GradientBoostingRegressor(), ExtraTreesRegressor()\n",
    "]\n",
    "\n",
    "# Define evaluation metrics\n",
    "evaluation_metrics = [\n",
    "    lambda y_true, y_pred: np.sqrt(metrics.mean_squared_error(y_true, y_pred)),\n",
    "    metrics.mean_squared_error, metrics.mean_absolute_error, metrics.r2_score,\n",
    "    metrics.mean_absolute_percentage_error, metrics.median_absolute_error\n",
    "]\n",
    "\n",
    "# Define a function to evaluate each model\n",
    "def evaluate_model(model, x_test, y_test, evaluation_metrics):\n",
    "    y_pred = model.predict(x_test)\n",
    "    scores = {}\n",
    "    for metric in evaluation_metrics:\n",
    "        name = metric.__name__\n",
    "        score = metric(y_test, y_pred)\n",
    "        scores[name] = score\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2fe4e1",
   "metadata": {},
   "source": [
    "#### <p style=\"text-align:left\"><span style=\"color:blue\">`Running` & `Evaluating` the Models</span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b454d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models and store results in a DataFrame\n",
    "results = []\n",
    "for model in models:\n",
    "    model_name = type(model).__name__\n",
    "    model.fit(x_train, y_train)\n",
    "    print(f\"✅ for the model '{model_name}'\")\n",
    "    scores = evaluate_model(model, x_test, y_test, evaluation_metrics)\n",
    "    results.append({'Model Name': model_name, **scores})\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Rename the <lambda> column to RMSE\n",
    "results_df = results_df.rename(columns={\"<lambda>\": \"root_mean_squared_error\"})\n",
    "\n",
    "\n",
    "winners = results_df.sort_values(\"root_mean_squared_error\", ascending=True)\n",
    "winner_name = f'{winners.iloc[0,0]}'\n",
    "winner_RMSE = f'{winners.iloc[0,1]:.2f}'\n",
    "winner_MSE = f'{winners.iloc[0,2]:.2f}'\n",
    "winner_MAE = f'{winners.iloc[0,3]:.2f}'\n",
    "winner_R2 = f'{winners.iloc[0,4]:.2f}'\n",
    "winners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048cd084",
   "metadata": {},
   "source": [
    "> For **better readability**, since I'm not using a framework like **[MLflow](https://mlflow.org/)** to assist us in **storing** the results of our experiments and **comparing the performance** of different models, I used for **`simple pandas dataframe`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42a17aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot and compare the different model perfs\n",
    "def plot_comparison(data, x_label, y_label, title):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(data['Model Name'], data[y_label])\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# root_mean_squared_error\n",
    "plot_comparison(winners, 'Model', 'root_mean_squared_error', 'Comparison of Root Mean Squared Error among Models')\n",
    "# r2_score\n",
    "plot_comparison(winners, 'Model', 'r2_score', 'Comparison of R2 Score among Models')\n",
    "# mean_absolute_error\n",
    "plot_comparison(winners, 'Model', 'mean_absolute_error', 'Comparison of MAE among Models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1568d50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 3 performing Models\n",
    "winner_models = [GradientBoostingRegressor(), \n",
    "                  ExtraTreesRegressor(), \n",
    "                  RandomForestRegressor()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f674b0",
   "metadata": {},
   "source": [
    "> * After obtaining the results, we deduced the **`top three performing models`**. These models demonstrated **incredible performance** even **withou**t the need for **feature engineering** or **model tuning**. However, it's important to note that in real-world scenarios, achieving such **exceptional results right from the start** often raises suspicions of **potential issues**, such as **data leakage**.\n",
    "\n",
    "> * To ensure the robustness of these models, we will compare their results using **`K-fold Cross Validation`**. This technique allows us to evaluate the models' performance on multiple subsets of the data, providing a more reliable estimation of their effectiveness. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ad2be",
   "metadata": {},
   "source": [
    "### <p style=\"text-align:left\"><span style=\"color:green\">Implementing `K-Fold Cross Validation`</span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90b995db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the number of folds\n",
    "k = 5\n",
    "\n",
    "# Initialize the KFold object\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize a list to store the evaluation scores for each fold\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "r2_scores = []\n",
    "mape_scores = []\n",
    "medae_scores = []\n",
    "\n",
    "# Optionally, initialize a list to store the predictions for each fold\n",
    "fold_predictions = []\n",
    "\n",
    "for model in winner_models:\n",
    "    # Iterate over the splits\n",
    "    for train_index, val_index in kf.split(x):\n",
    "        x_train, x_val = x.iloc[train_index], x.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        # Train your model using x_train and y_train\n",
    "        model = model\n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        # Evaluate your model using x_val and y_val\n",
    "        y_pred = model.predict(x_val)\n",
    "\n",
    "        mse_score = mean_squared_error(y_val, y_pred)\n",
    "        mae_score = mean_absolute_error(y_val, y_pred)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_val, y_pred)\n",
    "        medae = median_absolute_error(y_val, y_pred)\n",
    "\n",
    "        mse_scores.append(mse_score)\n",
    "        mae_scores.append(mae_score)\n",
    "        r2_scores.append(r2)\n",
    "        mape_scores.append(mape)\n",
    "        medae_scores.append(medae)\n",
    "\n",
    "        # Store the predictions for each fold if needed\n",
    "        fold_predictions.append(y_pred)\n",
    "\n",
    "    # Compute the average scores across all folds\n",
    "    average_mse = np.mean(mse_scores)\n",
    "    average_mae = np.mean(mae_scores)\n",
    "    average_r2 = np.mean(r2_scores)\n",
    "    average_mape = np.mean(mape_scores)\n",
    "    average_medae = np.mean(medae_scores)\n",
    "\n",
    "    # Print the average scores\n",
    "    print (f\"\\n >> Model Name: {type(model).__name__}\")\n",
    "    print(\"Average MSE:\", average_mse)\n",
    "    print(\"Average MAE:\", average_mae)\n",
    "    print(\"Average R-squared:\", average_r2)\n",
    "    print(\"Average MAPE:\", average_mape)\n",
    "    print(\"Average MedAE:\", average_medae)\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01edcc0",
   "metadata": {},
   "source": [
    "> * Based on the results obtained from the **5-fold cross-validation**, all models have demonstrated good performance. However, we i decided to select the **`GradientBoostingRegressor`** as the winning model for further analysis. This model exhibited the **best performance among the three, as indicated by the evaluation metrics**.\n",
    "\n",
    "> * To proceed with the analysis, we will train the **selected model** using all the data from **period_0**. By utilizing a larger dataset, i aim to enhance the model's ability to generalize and make accurate predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1f6ae5",
   "metadata": {},
   "source": [
    "### <p style=\"text-align:left\"><span style=\"color:green\">Training a model on `period_0 data` ( Obj 1 )</span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f24730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into features (x) and target variable (y)\n",
    "x = quarter_0.drop(target, axis=1)  # Extracting features by excluding the target variable\n",
    "y = quarter_0[target]  # Target variable\n",
    "\n",
    "# Instantiating the winner model (GradientBoostingRegressor) and fitting it to the data\n",
    "winner_model = GradientBoostingRegressor()\n",
    "winner_model.fit(x, y)\n",
    "winner_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9d5b18",
   "metadata": {},
   "source": [
    "> * Now that our model is trained on **period_0**, it has successfully passed the cross-validation test and achieved **impressive scores**. I would like to present this plot created using Plotly (the best data visualization tool in Python XD). \n",
    "\n",
    "> * I will now evaluate the model's performance on **x_test** to obtain the predicted values **y_pred** and compare them with **y_test**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30426006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecting x_test us\n",
    "y_pred = winner_model.predict(x_test)\n",
    "\n",
    "# Storing the results in a new DataFrame\n",
    "result = pd.DataFrame({'actual': y_test, 'pred': y_pred})\n",
    "result.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92b02e9",
   "metadata": {},
   "source": [
    "> * It is expected that the **predicted values** will closely **match** the **actual values**, given that the model has **already been trained** on and **performed well** on period_0 data. 😉\n",
    "\n",
    "> ⚠️ Please note that the **figures generated using Plotly may not render correctly on GitHub due to their dynamic nature**. \n",
    " \n",
    "> 💡 To ensure proper visualization, I have saved all the figures in the `results/` directory. You can easily download the HTML figures for viewing. Alternatively, you can run the notebook locally by referring to the detailed instructions provided in the **Read.me** file on my GitHub repository.\n",
    "\n",
    "> Additionally, I have hosted the figures using [HTML Github Preview](https://htmlpreview.github.io/). To explore the figures , simply click on the **`Explore Figure` Button**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6556f02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual Vs. Predicted plot\n",
    "fig = px.scatter(result, x='actual', y='pred', trendline='ols', color='actual', \n",
    "                 title='Actual Vs. Predicted', \n",
    "                 labels={'actual': 'Actual', 'pred': 'Predicted'})\n",
    "\n",
    "fig.add_trace(go.Scatter(x=result['actual'], y=result['actual'], mode='lines', \n",
    "                         name='Perfect fit', line=dict(color='green', width=2, dash='dash')))\n",
    "\n",
    "# Change the position of the legend to the bottom\n",
    "fig.update_layout(legend=dict(yanchor=\"bottom\", y=0.89, xanchor=\"left\", x=0.01))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Actual Vs. Predicted values', \n",
    "    title_x=0.6,  # Center the title\n",
    "    title_y=0.87,  \n",
    "    \n",
    "    annotations=[\n",
    "        {\n",
    "            \"x\": 0.01,\n",
    "            \"y\": 0.87 - i*0.06,\n",
    "            \"text\": f\"{label}: {value}\",\n",
    "            \"showarrow\": False,\n",
    "            \"font\": {\"size\": 12},\n",
    "            \"xref\": \"paper\",\n",
    "            \"yref\": \"paper\",\n",
    "        }\n",
    "        for i, (label, value) in enumerate(\n",
    "            [\n",
    "                (\"RMSE\", winner_RMSE),\n",
    "                (\"MSE\", winner_MSE),\n",
    "                (\"MAE\", winner_MAE),\n",
    "                (\"R2\", winner_R2),\n",
    "            ]\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea05235",
   "metadata": {},
   "source": [
    "[Explore Figure](https://htmlpreview.github.io/?https://github.com/labrijisaad/data-science-technical-test/blob/main/results/Actual%20vs.%20Predicted%20Trendline%20Plot%20-%20Train%20Data%20(period_0).html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de16e54",
   "metadata": {},
   "source": [
    "## <p style=\"text-align:center\"><span style=\"color:red\">Test `Winner Model` on `period_1/3 data` ( Obj 1 bis)</span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "480f7c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the datasets for periods 1, 2, and 3\n",
    "quarter_1 = periods[1]\n",
    "quarter_2 = periods[2]\n",
    "quarter_3 = periods[3]\n",
    "\n",
    "# Concatenating the data for periods 1 to 3\n",
    "later_quarters = pd.concat([quarter_1, quarter_2, quarter_3])\n",
    "print(f\"{later_quarters.shape =}\") # We expect a total number of rows 3000\n",
    "later_quarters.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1406f95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in later_quarters.columns:\n",
    "    if later_quarters[col].dtype=='object':\n",
    "        later_quarters[col] = le.fit_transform(later_quarters[col])\n",
    "        \n",
    "later_quarters.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ed2aa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into features (x) and target variable (y)\n",
    "x_later_quarters = later_quarters.drop(target, axis=1)  # Extracting features by excluding the target variable\n",
    "y_later_quarters = later_quarters[target]  # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58b09b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecting x_test us\n",
    "y_pred_later_quarters = winner_model.predict(x_later_quarters)\n",
    "\n",
    "# Storing the results in a new DataFrame\n",
    "result = pd.DataFrame({'actual': y_later_quarters, 'pred': y_pred_later_quarters})\n",
    "\n",
    "result.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4018a76d",
   "metadata": {},
   "source": [
    "> Judging from the `actual Vs. pred` head values alone, it is clear that the **model's performance on the later quarters' data is lacking**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3609a72",
   "metadata": {},
   "source": [
    "### <p style=\"text-align:left\"><span style=\"color:green\">`Actual Vs. Predicted values Plot` ( Obj 1 bis)</span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1cdf24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = result.actual.values.tolist()\n",
    "y_pred = result.pred.values.tolist()\n",
    "\n",
    "# Recalculate the Performance Metrics of the new model\n",
    "RMSE = mean_squared_error(y_actual, y_pred, squared=False)\n",
    "MSE = mean_squared_error(y_actual, y_pred)\n",
    "MAE = mean_absolute_error(y_actual, y_pred)\n",
    "R2 = r2_score(y_actual, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26725e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual Vs. Predicted plot\n",
    "fig = px.scatter(result, x='actual', y='pred', trendline='ols', color='actual', \n",
    "                 title='Actual Vs. Predicted', \n",
    "                 labels={'actual': 'Actual', 'pred': 'Predicted'})\n",
    "\n",
    "fig.add_trace(go.Scatter(x=result['actual'], y=result['actual'], mode='lines', \n",
    "                         name='Perfect fit', line=dict(color='green', width=2, dash='dash')))\n",
    "\n",
    "# Change the position of the legend to the bottom\n",
    "fig.update_layout(legend=dict(yanchor=\"bottom\", y=0.89, xanchor=\"left\", x=0.01))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Actual Vs. Predicted values', \n",
    "    title_x=0.6,  # Center the title\n",
    "    title_y=0.87,\n",
    "    annotations=[\n",
    "        {\n",
    "            \"x\": 0.01,\n",
    "            \"y\": 0.87 - i*0.06,\n",
    "            \"text\": f\"{label}: {value}\",\n",
    "            \"showarrow\": False,\n",
    "            \"font\": {\"size\": 12},\n",
    "            \"xref\": \"paper\",\n",
    "            \"yref\": \"paper\",\n",
    "        }\n",
    "        for i, (label, value) in enumerate(\n",
    "            [\n",
    "                (\"RMSE\", RMSE),\n",
    "                (\"MSE\", MSE),\n",
    "                (\"MAE\", MAE),\n",
    "                (\"R2\",R2),\n",
    "            ]\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee1b3bc",
   "metadata": {},
   "source": [
    "[Explore Figure](https://htmlpreview.github.io/?https://github.com/labrijisaad/data-science-technical-test/blob/main/results/Actual%20vs.%20Predicted%20Trendline%20Plot%20-%20New%20Data%20(period_1_2_3).html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b6de96",
   "metadata": {},
   "source": [
    "### <p style=\"text-align:left\"><span style=\"color:green\">`Distribution of Predicted Values vs. Actual Values` ( Obj 1 bis)</span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8683904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "y_actual = result.actual.values.tolist()\n",
    "y_pred = result.pred.values.tolist()\n",
    "\n",
    "# Create a histogram for 'y_pred' and 'y_actual'\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=y_pred, name='Actual', opacity=0.5))\n",
    "fig.add_trace(go.Histogram(x=y_actual, name='Predicted', opacity=0.5))\n",
    "\n",
    "# Configure layout and annotations\n",
    "fig.update_layout(\n",
    "    title='Distribution of Predicted Values vs. Actual Values',\n",
    "    xaxis_title='Values',\n",
    "    yaxis_title='Frequency',\n",
    "    barmode='overlay',\n",
    "    showlegend=True,\n",
    "    legend=dict(x=0.7, y=0.95),\n",
    "    \n",
    "    annotations=[\n",
    "        {\n",
    "            \"x\": 0.05,\n",
    "            \"y\": 0.97 - i*0.06,\n",
    "            \"text\": f\"{label}: {value}\",\n",
    "            \"showarrow\": False,\n",
    "            \"font\": {\"size\": 12},\n",
    "            \"xref\": \"paper\",\n",
    "            \"yref\": \"paper\",\n",
    "        }\n",
    "        for i, (label, value) in enumerate(\n",
    "            [\n",
    "                (\"RMSE\", RMSE),\n",
    "                (\"MSE\", MSE),\n",
    "                (\"MAE\", MAE),\n",
    "                (\"R2\",R2),\n",
    "            ]\n",
    "        )\n",
    "    ],\n",
    "    \n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886fc7fc",
   "metadata": {},
   "source": [
    "> * On this new data, it is evident that the **model does not perform well**. \n",
    "\n",
    "> * The performance metrics have **degraded**, and the distribution of the predicted values versus the actual values is similar in terms of shape, but they do not overlap. \n",
    "\n",
    "> * Therefore, we can conclude that there is a clear **`performance degradation`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac6fb38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f6af72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ce83ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4771473",
   "metadata": {},
   "outputs": [],
   "source": [
    "dddddd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4dc526",
   "metadata": {},
   "source": [
    "* **Entrainez votre modèle sur toute la période 0** puis **testez votre modèle** sur les données des **périodes 1, 2 et 3**. Comment évolue la performance du modèle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2766c1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insérez votre code ici\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0046afb",
   "metadata": {},
   "source": [
    "# Mesure de la dérive de variables catégorielles.\n",
    "\n",
    "Pour deux distributions de probabilités discrètes $P$ et $Q$, la **divergence de Kullback–Leibler** de $P$ par rapport à $Q$ est définie par:\n",
    "\n",
    "$$D_\\text{KL}(P \\parallel Q) = \\sum_{x\\in\\mathcal{X}} P(x) \\log\\left(\\frac{P(x)}{Q(x)}\\right)$$\n",
    "\n",
    "\n",
    "On définit la **divergence de Jensen-Shannon** comme une version lisse et symétrique de la divergence de Kullback-Leiber donnée par la formule suivante:\n",
    "\n",
    "$${\\rm JSD}(P \\parallel Q)= \\frac{1}{2}D(P \\parallel M)+\\frac{1}{2}D(Q \\parallel M)$$\n",
    "\n",
    "où $M=\\frac{1}{2}(P+Q)$\n",
    "\n",
    "### Exemple de calcul de $D_\\text{KL}(P \\parallel Q)$ : \n",
    "\n",
    "Soient $P$ =`[0.2, 0.3, 0.5]` et $Q$ =`[0.2, 0.4, 0.4]` deux vecteurs définissant une loi de probabilité discrete. Alors : \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "D_\\text{KL}(P \\parallel Q) & = P(0) \\log\\left(\\frac{P(0)}{Q(0)}\\right)\n",
    "                             + P(1) \\log\\left(\\frac{P(1)}{Q(1)}\\right)\n",
    "                             + P(2) \\log\\left(\\frac{P(2)}{Q(2)}\\right)\\\\ \\\\\n",
    "                           & = 0.2 \\log\\left(\\frac{0.2}{0.2}\\right)\n",
    "                             + 0.3 \\log\\left(\\frac{0.3}{0.4}\\right)\n",
    "                             + 0.5 \\log\\left(\\frac{0.5}{0.4}\\right) \\\\ \\\\\n",
    "                           & = 0.02526...\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "### Exercice\n",
    "\n",
    "* Implémenter des fonctions nommées **`KLDivergence(P, Q)`** et **`JSDivergence(P, Q)`** permettant de calculer les métriques définies ci-dessus.\n",
    "\n",
    "**Conseils**:\n",
    "* Utiliser la librairie Numpy permettant de facilement effectuer des calculs d'algèbre linéaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0564b5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "P = np.array([0.2, 0.3, 0.5])\n",
    "Q = np.array([0.2, 0.4, 0.4])\n",
    "\n",
    "def KLDivergence(P, Q):\n",
    "    \n",
    "    # Insérez votre code ici\n",
    "    \n",
    "    return None \n",
    "\n",
    "def JSDivergence(P, Q):\n",
    "    \n",
    "    # Insérez votre code ici\n",
    "    \n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d22e26f",
   "metadata": {},
   "source": [
    "* Lancer la cellule suivante pour tester votre fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297c6942",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"K-L Divergence:\", KLDivergence(P, Q))\n",
    "print(\"J-S Divergence:\", JSDivergence(P, Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa4c561",
   "metadata": {},
   "source": [
    "# Mesure de la dérive de variables quantitatives.\n",
    "\n",
    "Soit $P$ une mesure empirique d'échantillons $X_1, \\ldots, X_n\n",
    "$ et $Q$ une mesure empirique d'échantillons $Y_1, \\ldots, Y_n$, on définit la **Distance de Wasserstein** d'ordre $p$ par la fonction suivante :\n",
    "\n",
    "$$W_p(P, Q) = \\left( \\frac{1}{n}\\sum_{i=1}^n \\|X_{(i)} - Y_{(i)}\\|^p \\right)^{1/p}$$\n",
    "\n",
    "où $X_{(1)}, \\ldots, X_{(n)}$ et $Y_{(1)}, \\ldots, Y_{(n)}$ sont les [**statistiques d'ordre**](https://en.wikipedia.org/wiki/Order_statistic#Notation_and_examples) des échantillons $X$ et $Y$ et $p$ un nombre entier positif.\n",
    "\n",
    "### Exercice\n",
    "\n",
    "* Implémenter une fonction nommée **`WassersteinDistance(X, Y, p)`** permettant de calculer cette métrique à l'ordre **`p`** à partir de **deux échantillons** **`X`** et **`Y`** que l'on supposera de même longueur.\n",
    "\n",
    "**Conseils**:\n",
    "* Vous pouvez trier les échantillons pour obtenir les statistiques d'ordre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee91dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WassersteinDistance(X, Y, p):\n",
    "    \n",
    "    # Insérez votre code ici\n",
    "    \n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39f1958",
   "metadata": {},
   "source": [
    "* Lancer la cellule suivante pour tester votre fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a11c2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.beta(2, 6, 100)\n",
    "\n",
    "Y = np.random.beta(3, 6, 100)\n",
    "\n",
    "WassersteinDistance(X, Y, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9971506e",
   "metadata": {},
   "source": [
    "# Étude de la dérive du modèle.\n",
    "\n",
    "* En vous appuyant sur les métriques définies précédemment, illustrez et analysez visuellement la **dérive des données** ainsi que **la dérive du modèle** sur les périodes **1 à 3**. \n",
    "\n",
    "\n",
    "* Commenter les visualisations et déterminer si le modèle doit être mis à jour ou si sa performance est toujours acceptable. \n",
    "\n",
    "**Conseils**:\n",
    "* Vous serez évalué sur **la rigueur et le soin** que vous donnerez à votre analyse, et sur **vos capacités à synthéthiser votre étude**. Vous ne serez pas évalué sur vos conclusions.\n",
    "\n",
    "\n",
    "* Vous pouvez normaliser les données par rapport aux **moyennes et variances que vous calculerez sur la période 0** pour que les distances de Wasserstein pour différentes variables soient facilement comparables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9472419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insérez votre code ici\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
